{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('/Users/chandrasekharmukherjee/Home/Centrality framework/Modules')\n",
    "sys.path.append('I:/내 드라이브/backup/document/USC/Research/MCPC/Code/Codes')\n",
    "\n",
    "#Import packages\n",
    "#imports\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import sknetwork\n",
    "\n",
    "\n",
    "from sknetwork.ranking import PageRank\n",
    "from sknetwork.ranking import Betweenness\n",
    "from sknetwork.ranking import Closeness\n",
    "\n",
    "#import umap as umap\n",
    "\n",
    "from numba.typed import List\n",
    "import warnings\n",
    "from numba import njit\n",
    "import pynndescent\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import operator\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from random import randint\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import scipy\n",
    "#from umap.umap_ import *\n",
    "import math\n",
    "from random import randint\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import cifar100\n",
    "from keras.datasets import fashion_mnist\n",
    "import scanpy\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score, roc_auc_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
    "\n",
    "#import community as community_louvain\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "\n",
    "import scipy\n",
    "\n",
    "import igraph \n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import metric as met \n",
    "import embedding as embed   \n",
    "import data_utils_ch as data_util\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "\n",
    "import metric as met \n",
    "import embedding as embed   \n",
    "import data_utils_ch as data_util\n",
    "from subgraph import *\n",
    "\n",
    "\n",
    "met=importlib.reload(met)\n",
    "embed=importlib.reload(embed)\n",
    "data_util=importlib.reload(data_util)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing every dataset with top k % nodes induced subgraph (Louvain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top k% induced subgraph + Strong Majority Vote later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDF Export version (Final Working Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmRNA\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiRNA\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m survive\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m G, label\u001b[38;5;241m=\u001b[39m \u001b[43mdata_to_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43msurvive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#add self loop to nodes without outgoing edges\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes():\n",
      "File \u001b[1;32mi:\\내 드라이브\\backup\\document\\USC\\Research\\MCPC\\Code\\Codes\\New_Cleaned\\subgraph.py:126\u001b[0m, in \u001b[0;36mdata_to_graph\u001b[1;34m(name, survive)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#print(\"Dataset names is \",name,\" |V|, |E| #clusters= \",n,len(edge_list),len(set(label)))\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#These are for the bulk-RNA datasets.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmRNA\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmiRNA\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m#for survive in [0,1]:\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     edge_list,vlist,label,n\u001b[38;5;241m=\u001b[39m\u001b[43mdata_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_bulkRNA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43msurvive\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     label\u001b[38;5;241m=\u001b[39mdata_util\u001b[38;5;241m.\u001b[39mset_labels(label)\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m#print(\"Dataset names is \",name,\" |V|, |E| #clusters= \",n,len(edge_list),len(set(label)))\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#These contain image and document data.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m#datanames=['FashionMNIST','MNIST','seeds','breast-cancer','Omniglot','bbc_news','20NewsGroups','biorxiv','big_patent']\u001b[39;00m\n",
      "File \u001b[1;32mi:\\내 드라이브\\backup\\document\\USC\\Research\\MCPC\\Code\\Codes\\New_Cleaned\\data_utils_ch.py:650\u001b[0m, in \u001b[0;36mlocal_bulkRNA\u001b[1;34m(name, survive)\u001b[0m\n\u001b[0;32m    646\u001b[0m     Xdf\u001b[38;5;241m=\u001b[39mXdft\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    647\u001b[0m     dfl\u001b[38;5;241m=\u001b[39mdfsurvive\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 650\u001b[0m edge_list,vlist,label\u001b[38;5;241m=\u001b[39m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdfl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label)\n\u001b[0;32m    652\u001b[0m vlist\u001b[38;5;241m=\u001b[39m[i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)]\n",
      "File \u001b[1;32mi:\\내 드라이브\\backup\\document\\USC\\Research\\MCPC\\Code\\Codes\\New_Cleaned\\data_utils_ch.py:587\u001b[0m, in \u001b[0;36mprocess\u001b[1;34m(X, labels)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx0[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m10\u001b[39m])\n\u001b[0;32m    586\u001b[0m X1\u001b[38;5;241m=\u001b[39mX[idx0,:]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 587\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m idx\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import Cust_Top as Cust\n",
    "import networkx.algorithms.community.louvain as Louv\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as NMI\n",
    "from collections import deque\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import os\n",
    "from subgraph import * \n",
    "\n",
    "res_=[] #0.26, 0.24 is best for Louvain, 3.3, 2.0\n",
    "labels=[]\n",
    "total_labels=[]\n",
    "names = []\n",
    "\n",
    "data_names = ['mRNA','miRNA','seeds','breast-cancer','Omniglot','bbc_news','20NewsGroups_tfdif','biorxiv','big_patent','Citeseer','Eu core','FashionMNIST','MNIST','Cora','Cora full']\n",
    "\n",
    "# FR_ord = ['Rd', 'Ord']\n",
    "# FR_Recalc = ['FR_avg', 'FR_Re']\n",
    "FR_tp = ['FL','FL_ng','FL_ng_prop','PageRank']\n",
    "Num_hops = {0:'5',1:'log(n)'}\n",
    "Mod_tp = {0: 'Louv'}\n",
    "\n",
    "#top k% nodes for induced subgraph\n",
    "k=0.2\n",
    "traverse = {0:'Reset',1:'Static'}\n",
    "ord = {0:'Rand',1:'FR_order'}\n",
    "\n",
    "#list of plotted figures\n",
    "figures = []\n",
    "\n",
    "for data in data_names: \n",
    "    pdf_name = './Plots/'+data+'/'+data+'.pdf'\n",
    "    \n",
    "    for survive in [0]:\n",
    "        '''\n",
    "        Get the Graph from Edge_List and get the labels\n",
    "        '''\n",
    "        #DO not recalculate Graph for non bulk RNA datasets\n",
    "        if data not in ['mRNA','miRNA'] and survive==1:\n",
    "            continue\n",
    "        G, label= data_to_graph(data,survive)\n",
    "        #add self loop to nodes without outgoing edges\n",
    "        for node in G.nodes():\n",
    "            if not list(G.successors(node)):\n",
    "                G.add_edge(node, node, weight=1)\n",
    "        print('Number of nodes:',G.number_of_nodes())\n",
    "\n",
    "\n",
    "        data_directory = f'./Plots/{data}'\n",
    "        file_path = f'{data_directory}/{data}.txt'\n",
    "        os.makedirs(data_directory, exist_ok=True)\n",
    "        with open(file_path, 'a') as f:\n",
    "            f.write('------------------------------------------------------------------------\\n')\n",
    "        \n",
    "        for trav in [0]:\n",
    "            for node_ord in [1]:\n",
    "                for Num_hop in [1]:\n",
    "                    for res in [0.5,1,2]:\n",
    "                        for Mod_type in [0]:\n",
    "                            plt.figure() # figure for NMI\n",
    "                            plt.figure() # figure for purity\n",
    "                            plt.figure() # figure for Incoming Edge Degrees\n",
    "                            for FR_type in range(4):\n",
    "                                \n",
    "                                \n",
    "\n",
    "                                name =  data + ' (top ' + str(k*100) + '%): '+ str(FR_tp[FR_type]) + ' + '+ str(Num_hops[Num_hop]) +' hops'\n",
    "                                if FR_type==3:\n",
    "                                    name = data + ' (top ' + str(k*100) + '%): PageRank'\n",
    "                                if data in ['mRNA','miRNA']:\n",
    "                                    name = data +  ' (top ' + str(k*100) + '%): '+ str(FR_tp[FR_type]) + ' + '+ str(Num_hops[Num_hop]) +' hops' + ' + Survive: '+str(survive)\n",
    "                                    if FR_type==3:\n",
    "                                        name = data + ' (top ' + str(k*100) + '%): PageRank' + ' + Survive: '+str(survive)\n",
    "                                print(name)\n",
    "                                \n",
    "                                # Create the directory if it doesn't exist\n",
    "                                os.makedirs(data_directory, exist_ok=True)\n",
    "                                with open(file_path, 'a') as f:\n",
    "                                    f.write(name+'\\n')\n",
    "                                \n",
    "                                '''\n",
    "                                Get induced subgraph of top k% nodes in FR value\n",
    "                                '''\n",
    "                                #Get induced subgraph\n",
    "                                if Num_hop==0:\n",
    "                                    node2FR = calc_FlowRank(G, FR_type, 5)\n",
    "                                else:\n",
    "                                    node2FR = calc_FlowRank(G, FR_type, np.log2(G.number_of_nodes()))\n",
    "                                #Get top k% nodes\n",
    "                                node_list = sorted(G.nodes, key=lambda x: node2FR[x], reverse=True)\n",
    "                                H, node_ordered_by_FR = getInducedSubgraph(G, k, node_list) # H = induced subgraph\n",
    "\n",
    "\n",
    "\n",
    "                                '''\n",
    "                                Calculate NMI and Purity for different resolutions\n",
    "                                '''\n",
    "                                \n",
    "                                # partition = RC10.louvain_partitions(H, seed=0,resolution=res, FR_type=FR_type, Mod_type=Mod_type)\n",
    "                                partition = Louv.louvain_partitions(H, seed=0,resolution=res )\n",
    "                                partition_ = deque(partition, maxlen=1).pop()\n",
    "                                '''Run Louvain Again on the rest ( Warm Start )'''\n",
    "                                # k_ = int(k*len(node_list))\n",
    "                                # nodes_rest = node_list[k_:]\n",
    "                                # for node in nodes_rest:\n",
    "                                #     partition_.append({node})\n",
    "                                # label_ = part_to_full_label(partition_,G.number_of_nodes())\n",
    "                                # part_ = Cust.louvain_partitions(G, seed=0,resolution=res,init_part=partition_)\n",
    "                                # part = deque(part_, maxlen=1).pop()\n",
    "                                # label_ = part_to_full_label(part,G.number_of_nodes())\n",
    "\n",
    "\n",
    "                                '''\n",
    "                                Add nodes to the subgraph by strong majority vote\n",
    "                                '''\n",
    "                                #See the trend by adding node to the subgraph by strong majority vote (in order of FlowRank Value)\n",
    "                                H_label = part_to_full_label(partition_,G.number_of_nodes())\n",
    "                                #print('H_label: ',H_label, 'len: ',len(H_label))\n",
    "                                \n",
    "\n",
    "                                \n",
    "\n",
    "                                NMI_List, Purity_List, InEdge_List = merge_by_vote(trav, node_ord, node_ordered_by_FR,H_label, G, label)\n",
    "\n",
    "                                \n",
    "                                #Plot NMI_List\n",
    "                                fig_numbers = plt.get_fignums()\n",
    "                                nmi_fig =fig_numbers[-1]\n",
    "                                purity_fig = fig_numbers[-2]\n",
    "                                InEdge_fig = fig_numbers[-3]\n",
    "\n",
    "                                #plot NMI\n",
    "                                plt.figure(nmi_fig)\n",
    "                                plt.plot(NMI_List, label = str(FR_tp[FR_type]))\n",
    "                                with open(file_path, 'a') as f:\n",
    "                                    f.write('# of nodes not included: ' + str(G.number_of_nodes() - H.number_of_nodes() - len(NMI_List)+1) + '\\n')\n",
    "                                    f.write('NMI after top ' + str(k*100) + '% nodes: ' + str(round(NMI_List[0],3)) + '\\n')\n",
    "                                    f.write('Max NMI: ' + str(round(max(NMI_List),3)) + '\\n')\n",
    "                                print('Number of nodes not included: ' , G.number_of_nodes() - H.number_of_nodes() - len(NMI_List)+1) \n",
    "                                print('NMI after top ', k*100, '% nodes:', round(NMI_List[0],3))\n",
    "                                print('Max NMI: ', round(max(NMI_List),3))\n",
    "\n",
    "                                #plot Purity\n",
    "                                plt.figure(purity_fig)\n",
    "                                plt.plot(Purity_List, label = str(FR_tp[FR_type]))\n",
    "                                with open(file_path, 'a') as f:\n",
    "                                    f.write('Purity after top ' + str(k*100) + '% nodes: ' + str(round(Purity_List[0],3)) + '\\n')\n",
    "                                    f.write('Max Purity: ' + str(round(max(Purity_List),3)) + '\\n')\n",
    "\n",
    "                                #plot InEdge Degrees\n",
    "                                plt.figure(InEdge_fig)\n",
    "                                plt.plot(InEdge_List, label = str(FR_tp[FR_type]))\n",
    "                            \n",
    "                                \n",
    "                        plt.figure(nmi_fig)    \n",
    "                        #Plot all at once\n",
    "                        plt.xlabel('Number of nodes added after top ' + str(k*100) + '% nodes')\n",
    "                        plt.ylabel('NMI')\n",
    "                        plt.title(data + ' | top ' + str(k*100) + '%' + '| Num_hops: ' + str(Num_hops[Num_hop]) + '|res: ' + str(res)+ '|'\n",
    "                                +str(traverse[trav]) + ' | ' + str(ord[node_ord]))\n",
    "                        plt.legend()\n",
    "\n",
    "                        plt.figure(purity_fig)\n",
    "                        #Plot all at once\n",
    "                        plt.xlabel('Number of nodes added after top ' + str(k*100) + '% nodes')\n",
    "                        plt.ylabel('Purity')\n",
    "                        plt.title(data + ' | top ' + str(k*100) + '%' + '| Num_hops: ' + str(Num_hops[Num_hop]) + '|res: ' + str(res)+ '|'\n",
    "                                +str(traverse[trav]) + ' | ' + str(ord[node_ord]))\n",
    "                        plt.legend()\n",
    "\n",
    "                        plt.figure(InEdge_fig)\n",
    "                        #Plot all at once\n",
    "                        plt.xlabel('Number of nodes added after top ' + str(k*100) + '% nodes')\n",
    "                        plt.ylabel('InEdge Degrees')\n",
    "                        plt.title(data + ' | top ' + str(k*100) + '%' + '| Num_hops: ' + str(Num_hops[Num_hop]) + '|res: ' + str(res)+ '|'\n",
    "                                +str(traverse[trav]) + ' | ' + str(ord[node_ord]))\n",
    "                        plt.legend()\n",
    "    \n",
    "    p = PdfPages(pdf_name)            \n",
    "    fig_nums = plt.get_fignums() \n",
    "    figs = [plt.figure(n) for n in fig_nums] \n",
    "\t\n",
    "\t# iterating over the numbers in list \n",
    "    for fig in figs: \n",
    "\t\t# and saving the files \n",
    "        fig.savefig(p, format='pdf') \n",
    "\t\n",
    "\t# close the object \n",
    "    p.close() \n",
    "    plt.close('all')\n",
    "    \n",
    "        \n",
    "            \n",
    "\n",
    "                                \n",
    "                    \n",
    "\n",
    "                    \n",
    "                    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
